{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/benzilla987/ps-s3-ep22-eda-modeling-submission?scriptVersionId=144876409\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# Importing libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom scipy import stats\nfrom sklearn.cluster import KMeans\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import f1_score\n\n#Importing Data\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \ntrain = pd.read_csv('/kaggle/input/playground-series-s3e22/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s3e22/test.csv')\nsubmission = pd.read_csv('/kaggle/input/playground-series-s3e22/sample_submission.csv')\n\ntest.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-01T15:34:09.96945Z","iopub.execute_input":"2023-10-01T15:34:09.970545Z","iopub.status.idle":"2023-10-01T15:34:12.030758Z","shell.execute_reply.started":"2023-10-01T15:34:09.970502Z","shell.execute_reply":"2023-10-01T15:34:12.029571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Initial Exporatory Analysis","metadata":{}},{"cell_type":"code","source":"# Deaing with null values and converting categorical columns to appropriate datatype.\n# List of categorical columns to process\nnumeric_columns = train.select_dtypes(include=['int64', 'float64']).columns\ncategorical_columns = [\n    'temp_of_extremities',\n    'peripheral_pulse',\n    'mucous_membrane',\n    'capillary_refill_time',\n    'pain',\n    'peristalsis',\n    'abdominal_distention',\n    'nasogastric_tube',\n    'nasogastric_reflux',\n    'rectal_exam_feces',\n    'abdomen',\n    'abdomo_appearance',\n    'surgery',\n    'surgical_lesion',\n    'cp_data',\n    'outcome',\n    'age'\n]\n\n# Setting a custom color palette for Seaborn\ncustom_palette = sns.color_palette(\"Set2\")\n\n# Creating a square grid for all the plots\nnum_plots = len(categorical_columns) + len(numeric_columns) + 1  # Total number of plots\ngrid_size = int(num_plots ** 0.5) + 1  # Calculating the grid size\n\n# Creating a square grid of subplots\nfig, axes = plt.subplots(grid_size, grid_size, figsize=(15, 15))\n\n# Looping through the categorical columns\nfor i, column in enumerate(categorical_columns):\n    ax = axes[i // grid_size, i % grid_size]\n    sns.countplot(data=train, x=train[column], palette=custom_palette, ax=ax)\n    ax.set_title(f'Distribution of {column}', fontsize=10)\n    ax.set_xlabel(column, fontsize=8)\n    ax.set_ylabel('Count', fontsize=8)\n    ax.tick_params(axis='x', labelrotation=45, labelsize=6)\n    ax.tick_params(axis='y', labelsize=6)\n\n# Loop through the numeric columns \nfor i, column in enumerate(numeric_columns):\n    ax = axes[(i + len(categorical_columns)) // grid_size, (i + len(categorical_columns)) % grid_size]\n    sns.histplot(data=train, x=train[column], kde=True, color=custom_palette[2], ax=ax)\n    ax.set_title(f'Distribution of {column}', fontsize=10)\n    ax.set_xlabel(column, fontsize=8)\n    ax.set_ylabel('Frequency', fontsize=8)\n    ax.tick_params(axis='x', labelsize=6)\n    ax.tick_params(axis='y', labelsize=6)\n\n# Removing empty subplots\nfor i in range(num_plots, grid_size ** 2):\n    fig.delaxes(axes[i // grid_size, i % grid_size])\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-01T15:34:43.538212Z","iopub.execute_input":"2023-10-01T15:34:43.538921Z","iopub.status.idle":"2023-10-01T15:35:01.88356Z","shell.execute_reply.started":"2023-10-01T15:34:43.538885Z","shell.execute_reply":"2023-10-01T15:35:01.882108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Highest Score Yet\n## Submission 17\n## Score: 76.829","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport xgboost as xgb\nimport pandas as pd\nimport xgboost as xgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport pandas as pd\nfrom sklearn.ensemble import GradientBoostingClassifier  # Import the Gradient Boosting Classifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport pandas as pd\nfrom sklearn.ensemble import GradientBoostingClassifier  # Import the Gradient Boosting Classifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\n\n## Reading in the data\ntrain = pd.read_csv('/kaggle/input/playground-series-s3e22/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s3e22/test.csv')\n\n## Lesion 2 & Lession 2 values are not useful, dropping these variables because most values are = 0.\ntrain.drop('lesion_2', axis=1, inplace=True)\ntest.drop('lesion_2', axis=1, inplace=True)\ntrain.drop('lesion_3', axis=1, inplace=True)\ntest.drop('lesion_3', axis=1, inplace=True)\n\n# Dropping NA values from train df\ntrain.dropna(inplace=True)\n\n# Defining X/Y df objects for train/test\nX_train=train.drop('outcome', axis=1)\ny_train=train['outcome']\ny_train = y_train.map({'died': 0, 'euthanized': 1, 'lived': 2})\nX_test=test\n\n# Defining list of categorical columns\ncategorical_columns = [\n    'temp_of_extremities',\n    'peripheral_pulse',\n    'mucous_membrane',\n    'capillary_refill_time',\n    'pain',\n    'peristalsis',\n    'abdominal_distention',\n    'nasogastric_tube',\n    'nasogastric_reflux',\n    'rectal_exam_feces',\n    'abdomen',\n    'abdomo_appearance',\n    'surgery',\n    'surgical_lesion',\n    'cp_data',\n    'age'\n]\n\n# Seperating numerical columns from categorical columns\nnumerical_columns=[col for col in train.columns if col not in categorical_columns]\n\n# Combining data temporarily to one hot encode\ncombined_data = pd.concat([X_train, X_test], axis=0)\n\n# One hot encoding categorical columns\ncombined_encoded = pd.get_dummies(combined_data, columns=categorical_columns)\n\n# Splitting back up the training / testing data\nX_train_encoded = combined_encoded.iloc[:len(train)]\nX_test_encoded = combined_encoded.iloc[len(train):]\n\n# Creating and training the gb classifier\nxgb_classifier = xgb.XGBClassifier(n_estimators=300, random_state=42, learning_rate=0.1)\nxgb_classifier.fit(X_train_encoded, y_train)\n\n# Creating Predictions\npredictions = xgb_classifier.predict(X_test_encoded)\n\n# Creating a mapping dictionary\nlabel_mapping = {0: 'died', 1: 'euthanized', 2: 'lived'}\n\n# Applying the mapping to the predictions array\npredictions = np.vectorize(label_mapping.get)(predictions)\n\n# Appending predictions back onto test df\nX_test_encoded['outcome']=predictions\n\n# Building submission df\nsubmission_17=X_test_encoded[['id','outcome']]\n","metadata":{},"execution_count":null,"outputs":[]}]}