{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/benzilla987/ps-s3-ep22-eda-modeling-submission?scriptVersionId=143323028\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# Importing libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom scipy import stats\nfrom sklearn.cluster import KMeans\nfrom sklearn.ensemble import RandomForestClassifier\n\n#Importing Data\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \ntrain = pd.read_csv('/kaggle/input/playground-series-s3e22/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s3e22/test.csv')\nsubmission = pd.read_csv('/kaggle/input/playground-series-s3e22/sample_submission.csv')\n\ntrain.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-17T17:07:57.009016Z","iopub.execute_input":"2023-09-17T17:07:57.010688Z","iopub.status.idle":"2023-09-17T17:07:57.079196Z","shell.execute_reply.started":"2023-09-17T17:07:57.010637Z","shell.execute_reply":"2023-09-17T17:07:57.078034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Initial Exporatory Analysis","metadata":{}},{"cell_type":"code","source":"# Deaing with null values and converting categorical columns to appropriate datatype.\n# List of categorical columns to process\ncategorical_columns = [\n    'temp_of_extremities',\n    'peripheral_pulse',\n    'mucous_membrane',\n    'capillary_refill_time',\n    'pain',\n    'peristalsis',\n    'abdominal_distention',\n    'nasogastric_tube',\n    'nasogastric_reflux',\n    'rectal_exam_feces',\n    'abdomen',\n    'abdomo_appearance',\n    'surgery',\n    'surgical_lesion',\n    'cp_data',\n    'outcome',\n    'age'\n]\n\n# Loop through the selected categorical columns\nfor column in categorical_columns:\n    # Fill missing values with the mode\n    mode_value = train[column].mode()[0]\n    train[column].fillna(mode_value, inplace=True)\n    \n    # Convert the data type to categorical\n    train[column] = train[column].astype('category')\n\n## Dealing with outliers in numerical data\n\n## Rectal Temp\n# Calculate the Z-scores for 'rectal_temp'\nz_scores = np.abs(stats.zscore(train['rectal_temp']))\n\n# Define a threshold for identifying outliers (e.g., 3 standard deviations)\nthreshold = 3\n\n# Identify outliers based on the Z-scores\noutliers_mask = z_scores > threshold\n\n# Impute outliers with the mean value of 'rectal_temp'\nmean_rectal_temp = train['rectal_temp'].mean()\ntrain.loc[outliers_mask, 'rectal_temp'] = mean_rectal_temp\n\n# Optionally, you can create a new DataFrame with only the outliers\noutliers_df = train[outliers_mask]\n\n## Pulse\n# Calculate the 99th percentile value\npercentile_99 = np.percentile(train['pulse'], 99)\n\n# Replace values above the 99th percentile with the 99th percentile value\ntrain['pulse'][train['pulse'] > percentile_99] = percentile_99\n\n## Respiratory Rate\n# Calculate the 99th percentile value\npercentile_99 = np.percentile(train['respiratory_rate'], 99)\n\n# Replace values above the 99th percentile with the 99th percentile value\ntrain['respiratory_rate'][train['respiratory_rate'] > percentile_99] = percentile_99\n\n## Packed Cell Volume\n# Calculating the Z-scores for 'rectal_temp'\nz_scores = np.abs(stats.zscore(train['packed_cell_volume']))\n\n# Defining a threshold for identifying outliers (e.g., 3 standard deviations)\nthreshold = 3\n\n# Identifying outliers based on the Z-scores\noutliers_mask = z_scores > threshold\n\n# Imputing outliers with the mean value of 'rectal_temp'\nmean_rectal_temp = train['packed_cell_volume'].mean()\ntrain.loc[outliers_mask, 'packed_cell_volume'] = mean_rectal_temp\n\n## Total Protein\n# This distribution is broken up into clusters. So I'm using kmeans clustering to exmaine the distribution of the two clusters seperately\n\n# Defining a function to impute outliers in a cluster with the cluster's mean\ndef impute_outliers_with_mean(cluster_data):\n    cluster_mean = cluster_data.mean()\n    cluster_data[cluster_data < percentiles[0]] = cluster_mean\n    cluster_data[cluster_data > percentiles[4]] = cluster_mean\n    return cluster_data\n\n# Performing k-means clustering to identify clusters\nkmeans = KMeans(n_clusters=2)  # Adjust the number of clusters as needed\ntrain['cluster'] = kmeans.fit_predict(train[['total_protein']])\n\n# Iterating through each cluster\nfor cluster_id in train['cluster'].unique():\n    # Filtering the data for the current cluster\n    cluster_data = train[train['cluster'] == cluster_id]['total_protein']\n    \n    # Calculating and printing the percentiles for the cluster\n    percentiles = np.percentile(cluster_data, [25, 50, 75, 95, 99])\n    \n    # Identifying potential outliers within the cluster\n    potential_outliers = cluster_data[(cluster_data < percentiles[0]) | (cluster_data > percentiles[4])]\n    \n    # Imputing potential outliers with the cluster's mean\n    train.loc[train['cluster'] == cluster_id, 'total_protein'] = impute_outliers_with_mean(cluster_data)\n\n## Abdominal Proptein\n# Calculate the 99th percentile value\npercentile_99 = np.percentile(train['abdomo_protein'], 99)\n\n# Replace values above the 99th percentile with the 99th percentile value\ntrain['abdomo_protein'][train['abdomo_protein'] > percentile_99] = percentile_99\n\n## Lesion 1\n# Calculate the 99th percentile value\npercentile_99 = np.percentile(train['lesion_1'], 99)\n\n# Replace values above the 99th percentile with the 99th percentile value\ntrain['lesion_1'][train['lesion_1'] > percentile_99] = percentile_99\n\n## Lesion 2\n# Calculate the 99th percentile value\npercentile_99 = np.percentile(train['lesion_2'], 99)\n\n# Replace values above the 99th percentile with the 99th percentile value\ntrain['lesion_2'][train['lesion_2'] > percentile_99] = percentile_99\n# This replaces all values with zero. Upon closer examination, there are only 8 non-zero values in this column.\n# This suggests that this variable is not valuable in predictions.\n# Dropping the variable\ntrain.drop('lesion_2', axis=1, inplace=True)\n\n## The same issue is present with lesion_3, dropping this variable as well.\ntrain.drop('lesion_3', axis=1, inplace=True)\n    \n# Examining the shape of the data\nprint(\"Data Shape:\\n\")\nprint(train.shape,\"\\n\")\n# Summary statistics of numeric columns\nprint(\"Data Description:\\n\")\nprint(train.describe(),\"\\n\")\n\n# Data types and missing values\nprint(\"Data Types and Missing Values:\\n\")\nprint(train.info(),\"\\n\")\n\n# Count the number of unique values in each column\nprint(\"Number of unique values in each column:\\n\")\nprint(train.nunique(),\"\\n\")\n\n# Distribution of categorical variables\nprint(\"Distribution of categorical variables:\\n\")\ncategorical_columns = train.select_dtypes(include=['category']).columns\nfor column in categorical_columns:\n    plt.figure(figsize=(8, 6))\n    sns.countplot(data=train, x=train[column])\n    plt.title(f'Distribution of {column}')\n    plt.xticks(rotation=45)\n    plt.show()\n\n# Distribution of numeric variables\nnumeric_columns = train.select_dtypes(include=['int64', 'float64']).columns\nfor column in numeric_columns:\n    plt.figure(figsize=(8, 6))\n    sns.histplot(data=train, x=train[column], kde=True)\n    plt.title(f'Distribution of {column}')\n    plt.show()\n\n# Correlation matrix for numeric variables\ncorrelation_matrix = train[numeric_columns].corr()\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\nplt.title('Correlation Matrix')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-17T17:05:16.755226Z","iopub.execute_input":"2023-09-17T17:05:16.755683Z","iopub.status.idle":"2023-09-17T17:05:36.426706Z","shell.execute_reply.started":"2023-09-17T17:05:16.755648Z","shell.execute_reply":"2023-09-17T17:05:36.425515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.drop(columns=['cluster'], axis=1,inplace=True)\ntest.drop(columns=['lesion_2','lesion_3'], axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T17:05:44.404496Z","iopub.execute_input":"2023-09-17T17:05:44.40493Z","iopub.status.idle":"2023-09-17T17:05:44.418487Z","shell.execute_reply.started":"2023-09-17T17:05:44.404898Z","shell.execute_reply":"2023-09-17T17:05:44.416765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission 1 - Simple Decision Tree","metadata":{}},{"cell_type":"code","source":"# ## Defining the features and target variable for training data\n# X_train = train.drop(\"outcome\", axis=1)\n# y_train = train[\"outcome\"]\n\n# # Define the features for test data\n# X_test = test\n\n# # Handle categorical columns with one-hot encoding\n# categorical_columns = X_train.select_dtypes(include=['category']).columns\n\n# # Initialize the OneHotEncoder with handle_unknown='ignore'\n# encoder = OneHotEncoder(drop='first', sparse=False, handle_unknown='ignore')\n# X_train_encoded = pd.DataFrame(encoder.fit_transform(X_train[categorical_columns]))\n# X_test_encoded = pd.DataFrame(encoder.transform(X_test[categorical_columns]))\n\n# # Impute missing values\n# imputer = SimpleImputer(strategy='mean')\n# X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train.select_dtypes(exclude=['category'])))\n# X_test_imputed = pd.DataFrame(imputer.transform(X_test.select_dtypes(exclude=['category'])))\n\n# # Concatenate the encoded categorical columns and imputed numerical columns\n# X_train_processed = pd.concat([X_train_encoded, X_train_imputed], axis=1)\n# X_test_processed = pd.concat([X_test_encoded, X_test_imputed], axis=1)\n\n# # Create and train a Decision Tree classifier\n# clf = DecisionTreeClassifier(random_state=42)\n# clf.fit(X_train_processed, y_train)\n\n# # Make predictions on the test data\n# y_pred = clf.predict(X_test_processed)\n\n# # Create a DataFrame for the test predictions\n# submission_1 = pd.DataFrame({'id': test['id'], 'outcome': y_pred})\n\n# # Export the DataFrame to a CSV file\n# submission_1.to_csv('submission_1.csv', index=False)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-17T16:44:13.382269Z","iopub.execute_input":"2023-09-17T16:44:13.382729Z","iopub.status.idle":"2023-09-17T16:44:13.487945Z","shell.execute_reply.started":"2023-09-17T16:44:13.382694Z","shell.execute_reply":"2023-09-17T16:44:13.48636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission 2","metadata":{}},{"cell_type":"code","source":"## I went back and did some pre-processing. Then I hashed out my submission 1 code.\n## Gonna try to use random forest this time. With better pre-processing, I'm expecting\n## improved results. **fingers crossed!**\n\n# Define X_train and y_train from your training data\nX_train = train.drop('outcome', axis=1)  # Assuming 'outcome' is your target variable\ny_train = train['outcome']\n\n# Define X_test from your testing data\nX_test = test  # You can modify this according to your data preprocessing steps\n\n# Concatenate the training and testing datasets for one-hot encoding\ncombined_data = pd.concat([train, test], axis=0)\n\n# Perform one-hot encoding on the combined data\ncombined_data_encoded = pd.get_dummies(combined_data, columns=categorical_columns, drop_first=True)\n\n# Split the combined data back into training and testing datasets\nX_train_encoded = combined_data_encoded[:len(train)]\nX_test_encoded = combined_data_encoded[len(train):]\n\n# Creating a Random Forest classifier\nrf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n\n# Training the classifier on the training data\nrf_classifier.fit(X_train_encoded, y_train)\n\n# Making predictions on the test data\ny_pred = rf_classifier.predict(X_test_encoded)\n\n# Creating a DataFrame with 'id' and 'outcome' columns\nsubmission_2 = pd.DataFrame({'id': test['id'], 'outcome': y_pred})\n\n# Generating submission\nsubmission_2.to_csv('submission_2.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T17:06:19.082592Z","iopub.execute_input":"2023-09-17T17:06:19.083079Z","iopub.status.idle":"2023-09-17T17:06:19.52529Z","shell.execute_reply.started":"2023-09-17T17:06:19.083043Z","shell.execute_reply":"2023-09-17T17:06:19.52422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-09-17T16:48:40.827543Z","iopub.execute_input":"2023-09-17T16:48:40.827989Z","iopub.status.idle":"2023-09-17T16:48:40.839413Z","shell.execute_reply.started":"2023-09-17T16:48:40.827955Z","shell.execute_reply":"2023-09-17T16:48:40.837499Z"},"trusted":true},"execution_count":null,"outputs":[]}]}